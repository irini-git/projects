## Natural Language Processing

**NLP 01 Bigram BeRP**<br>
Bigram model to predict the conditional probability using data from The Berkeley Restaurant Project (BeRP).

**NLP 01 Bigram mini corpus**<br>
Bigram model to predict the conditional probability of the next word for French nurse.

**NLP 01 Good-Turing Smoothing**<br>
Good-Turing smoothing for catch of the day.

**NLP 01 Kneser-Ney Smoothing**<br>
Kneser-Ney Smoothing for Appeal by General de Gaulle.

**NLP 01 Minimum Edit distance**<br>
Minimum Edit distance : from manager to leader.

**NLP 01 SMS Spam Classifier**<br>
Spam or not spam : classify SMS messages using Naive Bayes.

**NLP 01 Type Token Ratio**<br>
Lexical variety within a text : calculate type-token ratio.

**NLP 02 Naive Bayes**<br>
Text Classification with Naive Bayes and Binary NB.

**NLP_02_Naive_Bayes_negation**
Text Classification with Naive Bayes and Binary NB : negation in samples.

**NLP 02 Spelling Correction Non-Word**<br>
Actress or across: Laplace (add one smoothing) for STM articles and speeches.

**NLP 02 Spelling Correction Real Word**<br>
Laplace (add one smoothing) for STM articles and X (twitter).



## Naming convention
Code : NLP for Natural Language Processing<br>
01 : difficulty of the concept<br>
Desctiption : high level description<br>

## License
Distributed under the MIT License. See [LICENSE](https://github.com/irini-git/projects/blob/main/LICENSE) for more information.
